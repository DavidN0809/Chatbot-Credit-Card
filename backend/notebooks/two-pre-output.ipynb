{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fd53f5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#https://www.kaggle.com/datasets/rikdifos/credit-card-approval-prediction\n",
    "#https://www.kaggle.com/datasets/alexisbcook/synthetic-credit-card-approval\n",
    "import os\n",
    "import kagglehub\n",
    "import warnings\n",
    "\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv(\"../dataset/credit-card-approval/credit_card_approval.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56be610d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.rename(columns={'TARGET': 'Approved'}, inplace=True)\n",
    "# Display the first few rows to confirm the change\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530349cd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "import colorama\n",
    "from colorama import Fore\n",
    "\n",
    "# Automatically identify categorical columns from the dataframe based on data types\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Identify potential categorical columns stored as numbers by checking unique values\n",
    "for col in df.columns:\n",
    "    if col != 'Approved':  # Skip 'Approved' column\n",
    "        if df[col].nunique() < 10 and col not in categorical_cols:  # Arbitrary threshold of < 10 unique values\n",
    "            categorical_cols.append(col)\n",
    "\n",
    "# Display identified categorical columns\n",
    "print(f\"Identified Categorical Columns (excluding 'Approved'): {categorical_cols}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf08666c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize a list to store bad predictors\n",
    "bad_predictors = []\n",
    "\n",
    "# You can change the threshold for what is considered a \"bad predictor\"\n",
    "p_value_threshold = 0.05  # Adjust this if needed\n",
    "\n",
    "# Evaluate each categorical column using chi-squared test\n",
    "for col in categorical_cols:\n",
    "    print(f\"{col}:\")\n",
    "    \n",
    "    # Perform chi-squared test\n",
    "    a = np.array(pd.crosstab(df['Approved'], df[col]))\n",
    "    stats, p, dof, _ = chi2_contingency(a, correction=False)\n",
    "    \n",
    "    if p > p_value_threshold:\n",
    "        print(Fore.RED + f\"'{col}' is a 'bad Predictor'\")\n",
    "        print(f\"p_val = {p}\\n\")\n",
    "        bad_predictors.append(col)  # Append the bad predictor to the list\n",
    "    else:\n",
    "        print(Fore.GREEN + f\"'{col}' is a 'Good Predictor'\")\n",
    "        print(f\"p_val = {p}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e120bd2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automatically drop bad predictors\n",
    "df.drop(bad_predictors, axis=1, inplace=True)\n",
    "\n",
    "# Print the final dataframe structure after dropping\n",
    "print(f\"Dropped bad predictors: {bad_predictors}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0649fb9c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip()  # Removes leading and trailing spaces from all column names\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Print columns to verify correctness\n",
    "print(\"Categorical columns:\", categorical_cols)\n",
    "print(\"Numerical columns:\", numerical_cols)\n",
    "\n",
    "# Remove 'Approved' if it's in either list\n",
    "if 'Approved' in categorical_cols:\n",
    "    categorical_cols.remove('Approved')\n",
    "if 'Approved' in numerical_cols:\n",
    "    numerical_cols.remove('Approved')\n",
    "\n",
    "# Check the results\n",
    "print(\"Categorical columns after removal:\", categorical_cols)\n",
    "print(\"Numerical columns after removal:\", numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040ccc55",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Data types of the columns:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nCategorical columns identified:\")\n",
    "print(categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304a2f27",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Initialize LabelEncoders for categorical variables\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    if col != 'Approved':  # Exclude the target column from encoding\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        label_encoders[col] = le  # Store the encoder for future use (e.g., inverse_transform)\n",
    "\n",
    "print(\"Unique values in categorical columns after encoding:\")\n",
    "for col in categorical_cols:\n",
    "    if col != 'Approved':  # Ensure target is not being encoded\n",
    "        print(f\"{col}: {df[col].unique()}\")\n",
    "print(\"\\nData types of the columns after encoding:\")\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6dfa1b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0753bc20",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming df is your DataFrame and 'Approved' is the target column\n",
    "\n",
    "# Select numerical features (excluding 'Approved')\n",
    "numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# Remove 'Approved' from the list of columns to scale\n",
    "if 'Approved' in numerical_cols:\n",
    "    numerical_cols.remove('Approved')\n",
    "\n",
    "# Scale only the numerical feature columns\n",
    "scaler = StandardScaler()\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = df.drop('Approved', axis=1)  # Drop 'Approved' from features\n",
    "y = df['Approved']  # Target variable\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting splits\n",
    "print(\"Training set:\", X_train.shape, y_train.shape)\n",
    "print(\"Test set:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5a6305",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df['Approved'].value_counts())  # Ensure the target has values like 0/1 or yes/no\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693cba15",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d49c4b7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reduce dataset size using stratified sampling (optional)\n",
    "# X_sample, _, y_sample, _ = train_test_split(X, y, train_size=0.1, stratify=y, random_state=42)\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6b3e3f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, roc_auc_score, f1_score\n",
    "\n",
    "# Set up K-Fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# List of models to evaluate with optimizations\n",
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42, ccp_alpha=0.01, max_depth=6, min_samples_split=20),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42, n_estimators=50, max_depth=6, n_jobs=-1, min_samples_split=10, max_features='sqrt'),\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000, C=0.5, n_jobs=-1),  # Added regularization with C=0.5\n",
    "    \"XGBoost\": XGBClassifier(random_state=42, eval_metric='logloss', tree_method='gpu_hist', n_estimators=50, max_depth=6, subsample=0.8, colsample_bytree=0.8, reg_lambda=10)  # Added L2 regularization with reg_lambda=10\n",
    "}\n",
    "\n",
    "# Dictionary to store evaluation results\n",
    "results = {}\n",
    "\n",
    "# Iterate over each model with cross-validation\n",
    "for model_name, model in models.items():\n",
    "    # Cross-validate the model\n",
    "    cv_accuracy = cross_val_score(model, X_train, y_train, cv=kf, scoring='accuracy').mean()\n",
    "    cv_f1 = cross_val_score(model, X_train, y_train, cv=kf, scoring='f1').mean()\n",
    "    cv_auc = cross_val_score(model, X_train, y_train, cv=kf, scoring='roc_auc').mean()\n",
    "    cv_mae = cross_val_score(model, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error').mean() * -1  # Convert negative MAE to positive\n",
    "    \n",
    "    # Train the model on the full training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # For AUC-ROC, we need the probability scores for the positive class\n",
    "    y_test_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "    test_auc = roc_auc_score(y_test, y_test_prob) if y_test_prob is not None else None\n",
    "    \n",
    "    # Check if any metric is 1.0, and skip such models\n",
    "    if test_accuracy == 1.0 or test_auc == 1.0 or test_f1 == 1.0 or test_mae == 0.0:\n",
    "        print(f\"{model_name} is overfitting with a metric of 1.0, skipping this model.\")\n",
    "        continue\n",
    "    \n",
    "    # Store the evaluation results\n",
    "    results[model_name] = {\n",
    "        \"CV Accuracy\": cv_accuracy,\n",
    "        \"CV AUC\": cv_auc,\n",
    "        \"CV MAE\": cv_mae,\n",
    "        \"CV F1\": cv_f1,\n",
    "        \"Test Accuracy\": test_accuracy,\n",
    "        \"Test AUC\": test_auc,\n",
    "        \"Test MAE\": test_mae,\n",
    "        \"Test F1\": test_f1\n",
    "    }\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nModel Evaluation Results:\")\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\n{model_name} Evaluation:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Filter out models with any 1.0 metric, if any are left\n",
    "filtered_results = {model: metrics for model, metrics in results.items() \n",
    "                    if all(metric != 1.0 for metric in metrics.values())}\n",
    "\n",
    "# If no models remain after filtering, display a message\n",
    "if not filtered_results:\n",
    "    print(\"\\nNo models remain after filtering out overfitting models with metrics of 1.0.\")\n",
    "else:\n",
    "    # Select the best model based on the balance of metrics (accuracy, AUC, F1, MAE)\n",
    "    best_model = max(filtered_results, key=lambda x: (\n",
    "        filtered_results[x][\"Test Accuracy\"] + \n",
    "        filtered_results[x][\"Test AUC\"] + \n",
    "        filtered_results[x][\"Test F1\"] - \n",
    "        filtered_results[x][\"Test MAE\"]\n",
    "    ))  # Higher Accuracy, AUC, F1 and lower MAE are preferred\n",
    "\n",
    "    print(f\"\\nBest Model: {best_model}\")\n",
    "    print(f\"Best Model Metrics: {filtered_results[best_model]}\")\n",
    "\n",
    "    # Access the trained model object\n",
    "    best_model_name = best_model\n",
    "    best_model = models[best_model_name]  # This is the trained best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab22f035",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from LIME (optional)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Define the LimeTabularExplainer\n",
    "lime_explainer = LimeTabularExplainer(\n",
    "    training_data=np.array(X_train),\n",
    "    feature_names=X_train.columns,\n",
    "    class_names=['Not Approved', 'Approved'],\n",
    "    mode='classification'\n",
    ")\n",
    "# Initialize an empty list to store explanations\n",
    "explanations = []\n",
    "\n",
    "\n",
    "# Suppress the specific warnings from the deprecated usage in lime\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Define the LimeTabularExplainer\n",
    "lime_explainer = LimeTabularExplainer(\n",
    "    training_data=np.array(X_train),\n",
    "    feature_names=X_train.columns,\n",
    "    class_names=['Not Approved', 'Approved'],  # Adjust according to your dataset\n",
    "    mode='classification'\n",
    ")\n",
    "\n",
    "# Pick a single instance from the test set to explain for testing\n",
    "i = 0  # Index of the instance to explain\n",
    "\n",
    "exp = lime_explainer.explain_instance(X_test.iloc[i], best_model.predict_proba)\n",
    "\n",
    "# Display the explanation for the instance\n",
    "exp.show_in_notebook(show_table=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0087f344",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Suppress specific warnings from LIME (optional)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Function to extract the feature name from the LIME explanation\n",
    "def extract_feature_name(feature_str):\n",
    "    # Split on '<', '>', '=', '!=', etc., and take the first part\n",
    "    split_symbols = ['<=', '>=', '<', '>', '!=', '=']\n",
    "    for symbol in split_symbols:\n",
    "        if symbol in feature_str:\n",
    "            feature_name = feature_str.split(symbol)[0].strip()\n",
    "            break\n",
    "    else:\n",
    "        feature_name = feature_str.strip()\n",
    "    # Remove any numeric values\n",
    "    feature_name = re.sub(r'[0-9]+', '', feature_name)\n",
    "    # Remove any non-alphanumeric characters except spaces\n",
    "    feature_name = re.sub(r'[^A-Za-z0-9 ]+', '', feature_name)\n",
    "    # Replace multiple spaces with a single space\n",
    "    feature_name = re.sub(r'\\s+', ' ', feature_name)\n",
    "    return feature_name.strip()\n",
    "\n",
    "# Initialize the LimeTabularExplainer for the entire feature dataset `X`\n",
    "lime_explainer = LimeTabularExplainer(\n",
    "    training_data=np.array(X),\n",
    "    feature_names=X.columns,\n",
    "    class_names=['Not Approved', 'Approved'],  # Adjust according to your dataset\n",
    "    mode='classification'\n",
    ")\n",
    "\n",
    "# Initialize an empty list to store explanations\n",
    "explanations = []\n",
    "\n",
    "# Loop over the instances in the entire feature dataset `X`\n",
    "for idx, instance in X.iterrows():\n",
    "    # Get the model's prediction\n",
    "    prediction = best_model.predict(instance.values.reshape(1, -1))[0]\n",
    "    # Get the actual label from `y`\n",
    "    actual_label = y.loc[idx]\n",
    "    # Generate LIME explanation\n",
    "    exp = lime_explainer.explain_instance(\n",
    "        data_row=instance,\n",
    "        predict_fn=best_model.predict_proba,\n",
    "        num_features=len(X.columns)  # Consider all features\n",
    "    )\n",
    "    # Get the list of (feature, contribution) pairs\n",
    "    explanation_list = exp.as_list()\n",
    "\n",
    "    # Determine the decision (Approved or Denied)\n",
    "    decision = 'Approved' if prediction == 1 else 'Denied'\n",
    "\n",
    "    # Collect features contributing to the predicted class\n",
    "    features_contributing = []\n",
    "    for feature, weight in explanation_list:\n",
    "        # For the predicted class, collect all features regardless of weight\n",
    "        if (prediction == 1 and weight > 0) or (prediction == 0 and weight < 0):\n",
    "            feature_name = extract_feature_name(feature)\n",
    "            if feature_name and feature_name not in features_contributing:\n",
    "                features_contributing.append(feature_name)\n",
    "\n",
    "    # Construct the explanation text\n",
    "    if features_contributing:\n",
    "        explanation_text = f\"This application was {decision.lower()} due to \" + \", \".join(features_contributing) + \".\"\n",
    "    else:\n",
    "        explanation_text = f\"This application was {decision.lower()}.\"\n",
    "\n",
    "    # Append the explanation and related information\n",
    "    explanations.append({\n",
    "        'Index': idx,\n",
    "        'Prediction': decision,\n",
    "        'Actual': 'Approved' if actual_label == 1 else 'Denied',\n",
    "        'Explanation': explanation_text\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618a7f1e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the explanations to a DataFrame\n",
    "explanations_df = pd.DataFrame(explanations)\n",
    "\n",
    "# Output the first explanation\n",
    "first_explanation = explanations_df.iloc[0]\n",
    "\n",
    "# Print the details\n",
    "print(\"Index:\", first_explanation['Index'])\n",
    "print(\"Prediction:\", first_explanation['Prediction'])\n",
    "print(\"Actual:\", first_explanation['Actual'])\n",
    "print(\"Explanation:\", first_explanation['Explanation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0f1944",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "explanations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc03637",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reimport data due to earlier augmentation\n",
    "\n",
    "df = pd.read_csv(\"../dataset/credit-card-approval/credit_card_approval.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e99846",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add the 'Explanation' column (Reason) from explanations_df to the original df\n",
    "df['Reason'] = explanations_df['Explanation']\n",
    "\n",
    "# Save the updated dataframe with the new column as a CSV file\n",
    "df.to_csv('../dataset/target-augmented.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d715bdd6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b94665",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CC-Chatbot)",
   "language": "python",
   "name": "cc-chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 0.608313,
   "end_time": "2024-10-25T02:09:54.040588",
   "environment_variables": {},
   "exception": null,
   "input_path": "Pre-processing-Copy2.ipynb",
   "output_path": "two-pre-output.ipynb",
   "parameters": {},
   "start_time": "2024-10-25T02:09:53.432275",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}