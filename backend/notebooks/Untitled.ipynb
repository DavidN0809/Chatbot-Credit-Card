{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd16942a-1426-407f-85b4-9edb1a3ae726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2c15e2f4c249f3985c3e5e6477dbb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the PEFT configuration...\n",
      "Loading the PEFT model...\n",
      "Model loaded successfully.\n",
      "Tokenizer vocabulary size: 128256\n",
      "Model's embedding size: 128256\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "\n",
    "# Define the paths\n",
    "trained_model_path = \"/opt/notebooks/Chatbot-Credit-Card/backend/models/llama-3.1-8b-CC/base-final\"\n",
    "base_model_path = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "# BitsAndBytes configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load the tokenizer from the trained model path\n",
    "tokenizer = AutoTokenizer.from_pretrained(trained_model_path, trust_remote_code=True)\n",
    "\n",
    "# Do not add special tokens or resize embeddings here\n",
    "\n",
    "# Load the base model\n",
    "print(\"Loading the base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load PEFT configuration\n",
    "print(\"Loading the PEFT configuration...\")\n",
    "peft_config = PeftConfig.from_pretrained(trained_model_path)\n",
    "\n",
    "# Load the PEFT model\n",
    "print(\"Loading the PEFT model...\")\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    trained_model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Verify that the tokenizer and model's embeddings are aligned\n",
    "print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Model's embedding size: {model.get_input_embeddings().weight.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92e81118-98ca-493b-ae0b-7892496e3058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the prompt\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"My age is 30, income is $40,000, and credit score is 580.\"}\n",
    "]\n",
    "\n",
    "def format_chat_messages(messages):\n",
    "    formatted_prompt = \"\"\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"user\":\n",
    "            formatted_prompt += \"<start_input>\\n\"\n",
    "            formatted_prompt += f\"{message['content']}\\n\"\n",
    "            formatted_prompt += \"<end_input>\\n\"\n",
    "    formatted_prompt += \"<start_label>\\n\"  # Indicate that assistant's response should follow\n",
    "    return formatted_prompt\n",
    "\n",
    "prompt = format_chat_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27813f93-03da-48ef-bd4b-03f87f812759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "<start_input>\n",
      "My age is 30, income is $40,000, and credit score is 580.\n",
      "<end_input>\n",
      "<start_label>\n",
      "Age: 30 years old, Income: $40,000.0, Credit score: 580.0\n",
      "<end_label>\n",
      "<start_prediction>\n",
      "Income: $40,500.0, Credit score: 585.0\n",
      "<end_prediction>\n",
      "<start_output>\n",
      "Income: 40500.0, Credit score: 585.0\n",
      "<end_output>\n",
      "<start_code>\n",
      "age = 30.0\n",
      "income = 40000.0\n",
      "credit_score = 580.0\n",
      "income_prediction = 40500.0\n",
      "credit_score_prediction = 585.0\n",
      "<end_code>\n",
      "<start_markdown>\n",
      "# Income and Credit Score Prediction\n",
      "This is a machine learning project to predict income and credit score\n"
     ]
    }
   ],
   "source": [
    "# Generate response from the model\n",
    "print(\"Generating response...\")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")  # Ensure the input is moved to the correct device (e.g., GPU)\n",
    "\n",
    "# Generate tokens with the model\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=150,  # Adjust based on how long you want the output\n",
    "    do_sample=True,      # Enable sampling for more varied responses\n",
    "    temperature=0.7,     # Adjust for response diversity (lower for more deterministic output)\n",
    "    top_k=50,            # Consider the top-k tokens for sampling\n",
    ")\n",
    "\n",
    "# Decode the generated tokens to text\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the response\n",
    "print(\"Response:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5e04c1-e96b-4316-b958-ac63ce58cdd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CC-Chatbot)",
   "language": "python",
   "name": "cc-chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
