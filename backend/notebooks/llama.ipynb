{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb4e95d4-31a1-4aae-9b17-0accf2c76a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import os, torch, wandb\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, setup_chat_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae67b3f7-d781-4d5c-a92c-dbf4bbdf463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface-cli login --token key   \n",
    "# wandb login --relogin key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22140ac5-0ef7-42df-9eb0-d1d5f8def36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdnicho26\u001b[0m (\u001b[33mdnicho26-university-of-north-carolina-at-charlotte\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/opt/notebooks/Chatbot-Credit-Card/backend/notebooks/wandb/run-20241111_143130-ufezt1ua</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dnicho26-university-of-north-carolina-at-charlotte/Fine-tune%20Llama%203%20on%20CC%20Dataset/runs/ufezt1ua' target=\"_blank\">rare-meadow-33</a></strong> to <a href='https://wandb.ai/dnicho26-university-of-north-carolina-at-charlotte/Fine-tune%20Llama%203%20on%20CC%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dnicho26-university-of-north-carolina-at-charlotte/Fine-tune%20Llama%203%20on%20CC%20Dataset' target=\"_blank\">https://wandb.ai/dnicho26-university-of-north-carolina-at-charlotte/Fine-tune%20Llama%203%20on%20CC%20Dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dnicho26-university-of-north-carolina-at-charlotte/Fine-tune%20Llama%203%20on%20CC%20Dataset/runs/ufezt1ua' target=\"_blank\">https://wandb.ai/dnicho26-university-of-north-carolina-at-charlotte/Fine-tune%20Llama%203%20on%20CC%20Dataset/runs/ufezt1ua</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project='Fine-tune Llama 3 on CC Dataset', \n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06c9fc4d-7637-4dc9-b335-5733a304c594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Client Number</th>\n",
       "      <th>Car Ownership</th>\n",
       "      <th>Property Ownership</th>\n",
       "      <th>Number of Children</th>\n",
       "      <th>Annual Income</th>\n",
       "      <th>Income Category</th>\n",
       "      <th>Education Level</th>\n",
       "      <th>Marital Status</th>\n",
       "      <th>Housing Type</th>\n",
       "      <th>Age (Days)</th>\n",
       "      <th>Employment Duration (Days)</th>\n",
       "      <th>Work Phone</th>\n",
       "      <th>Phone</th>\n",
       "      <th>Email</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Family Size</th>\n",
       "      <th>Approved</th>\n",
       "      <th>Reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5008804</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>427500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>32 years old</td>\n",
       "      <td>4542.0 days</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>This application was approved due to Client Nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5008805</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>427500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>32 years old</td>\n",
       "      <td>4542.0 days</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>This application was approved due to Client Nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5008806</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58 years old</td>\n",
       "      <td>1134.0 days</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>This application was approved due to Client Nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5008808</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>52 years old</td>\n",
       "      <td>3051.0 days</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>This application was approved due to Client Nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5008809</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>52 years old</td>\n",
       "      <td>3051.0 days</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>This application was approved due to Client Nu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Client Number  Car Ownership  Property Ownership  Number of Children  \\\n",
       "0        5008804              1                   1                   0   \n",
       "1        5008805              1                   1                   0   \n",
       "2        5008806              1                   1                   0   \n",
       "3        5008808              0                   1                   0   \n",
       "4        5008809              0                   1                   0   \n",
       "\n",
       "   Annual Income  Income Category  Education Level  Marital Status  \\\n",
       "0       427500.0                4                1               0   \n",
       "1       427500.0                4                1               0   \n",
       "2       112500.0                4                5               1   \n",
       "3       270000.0                0                5               3   \n",
       "4       270000.0                0                5               3   \n",
       "\n",
       "   Housing Type    Age (Days) Employment Duration (Days)  Work Phone  Phone  \\\n",
       "0             4  32 years old                4542.0 days         1.0    0.0   \n",
       "1             4  32 years old                4542.0 days         1.0    0.0   \n",
       "2             1  58 years old                1134.0 days         0.0    0.0   \n",
       "3             1  52 years old                3051.0 days         0.0    1.0   \n",
       "4             1  52 years old                3051.0 days         0.0    1.0   \n",
       "\n",
       "   Email  Occupation  Family Size  Approved  \\\n",
       "0    0.0          18          2.0         1   \n",
       "1    0.0          18          2.0         1   \n",
       "2    0.0          16          2.0         1   \n",
       "3    1.0          14          1.0         1   \n",
       "4    1.0          14          1.0         1   \n",
       "\n",
       "                                              Reason  \n",
       "0  This application was approved due to Client Nu...  \n",
       "1  This application was approved due to Client Nu...  \n",
       "2  This application was approved due to Client Nu...  \n",
       "3  This application was approved due to Client Nu...  \n",
       "4  This application was approved due to Client Nu...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/opt/notebooks/Chatbot-Credit-Card/backend/dataset/target-augmented.csv')\n",
    "# Post-Training Augmentation (for interpretability)\n",
    "# Convert Age (Years) to string for user-friendly output\n",
    "df['Age (Days)'] = df['Age (Days)'].astype(str) + \" years old\"\n",
    "\n",
    "# Ensure Employment Duration (Days) is numeric\n",
    "df['Employment Duration (Days)'] = pd.to_numeric(df['Employment Duration (Days)'], errors='coerce')\n",
    "# Fill NaN values or handle invalid data\n",
    "df['Employment Duration (Days)'].fillna(0, inplace=True)  # Assume NaN means unemployed\n",
    "# Convert Employment Duration to human-readable strings\n",
    "df['Employment Duration (Days)'] = df['Employment Duration (Days)'].apply(\n",
    "    lambda x: f\"{x} days\" if x > 0 else \"Unemployed\"\n",
    ")\n",
    "\n",
    "# Convert Months Balance to human-readable format\n",
    "# df['begin_month'] = df['begin_month'].astype(str) + \" months ago\"\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcdf9bec-3a1a-44d7-a003-001201de0cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;gender&gt; Female &lt;/gender&gt; &lt;age&gt; 32 years old d...</td>\n",
       "      <td>&lt;approved&gt; Yes &lt;/approved&gt; &lt;reason&gt; This appli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;gender&gt; Female &lt;/gender&gt; &lt;age&gt; 32 years old d...</td>\n",
       "      <td>&lt;approved&gt; Yes &lt;/approved&gt; &lt;reason&gt; This appli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;gender&gt; Female &lt;/gender&gt; &lt;age&gt; 58 years old d...</td>\n",
       "      <td>&lt;approved&gt; Yes &lt;/approved&gt; &lt;reason&gt; This appli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;gender&gt; Female &lt;/gender&gt; &lt;age&gt; 52 years old d...</td>\n",
       "      <td>&lt;approved&gt; Yes &lt;/approved&gt; &lt;reason&gt; This appli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;gender&gt; Female &lt;/gender&gt; &lt;age&gt; 52 years old d...</td>\n",
       "      <td>&lt;approved&gt; Yes &lt;/approved&gt; &lt;reason&gt; This appli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  <gender> Female </gender> <age> 32 years old d...   \n",
       "1  <gender> Female </gender> <age> 32 years old d...   \n",
       "2  <gender> Female </gender> <age> 58 years old d...   \n",
       "3  <gender> Female </gender> <age> 52 years old d...   \n",
       "4  <gender> Female </gender> <age> 52 years old d...   \n",
       "\n",
       "                                               label  \n",
       "0  <approved> Yes </approved> <reason> This appli...  \n",
       "1  <approved> Yes </approved> <reason> This appli...  \n",
       "2  <approved> Yes </approved> <reason> This appli...  \n",
       "3  <approved> Yes </approved> <reason> This appli...  \n",
       "4  <approved> Yes </approved> <reason> This appli...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to preprocess the data with special tokens for fine-tuning LLaMA3\n",
    "def preprocess_data_generalized(row):\n",
    "    try:\n",
    "        # Generate text for each input feature with special tokens, handling missing values\n",
    "        gender_text = f\"<gender> {'Male' if row.get('Gender', 0) == 1 else 'Female'} </gender>\"\n",
    "        age_text = f\"<age> {row.get('Age (Days)', 'Unknown')} days </age>\"\n",
    "        debt_text = f\"<debt> {row.get('Debt', '0')} units </debt>\"\n",
    "        married_text = f\"<married> {'Yes' if row.get('Marital Status', 0) == 1 else 'No'} </married>\"\n",
    "        bank_customer_text = f\"<bank_customer> {'Yes' if row.get('BankCustomer', 0) == 1 else 'No'} </bank_customer>\"\n",
    "        industry_text = f\"<industry> {row.get('Occupation', 'Unknown')} </industry>\"\n",
    "        ethnicity_text = f\"<ethnicity> {row.get('Ethnicity', 'Unknown')} </ethnicity>\"\n",
    "        years_employed_text = f\"<years_employed> {row.get('Employment Duration (Days)', '0')} days </years_employed>\"\n",
    "        prior_default_text = f\"<prior_default> {'Yes' if row.get('PriorDefault', 0) == 1 else 'No'} </prior_default>\"\n",
    "        employed_text = f\"<employed> {'Yes' if row.get('Employed', 0) == 1 else 'No'} </employed>\"\n",
    "        credit_score_text = f\"<credit_score> {row.get('CreditScore', 'Unknown')} </credit_score>\"\n",
    "        drivers_license_text = f\"<drivers_license> {'Yes' if row.get('Work Phone', 0) == 1 else 'No'} </drivers_license>\"\n",
    "        citizen_text = f\"<citizen> {row.get('Citizen', 'Unknown')} </citizen>\"\n",
    "        zip_code_text = f\"<zip_code> {row.get('ZipCode', 'Unknown')} </zip_code>\"\n",
    "        income_text = f\"<income> {row.get('Annual Income', '0')} units </income>\"\n",
    "\n",
    "        # Combine all input text with special tokens\n",
    "        input_text = \" \".join([\n",
    "            gender_text, age_text, debt_text, married_text, bank_customer_text,\n",
    "            industry_text, ethnicity_text, years_employed_text, prior_default_text,\n",
    "            employed_text, credit_score_text, drivers_license_text, citizen_text,\n",
    "            zip_code_text, income_text\n",
    "        ])\n",
    "\n",
    "        # Output format for LLaMA fine-tuning (using special tokens for labels)\n",
    "        output_text = f\"<approved> {'Yes' if row.get('Approved', 0) == 1 else 'No'} </approved> <reason> {row.get('Reason', 'Unknown')} </reason>\"\n",
    "\n",
    "        return {\"text\": input_text, \"label\": output_text}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {row.to_dict()} - {e}\")\n",
    "        return None\n",
    "\n",
    "# Assume df is your original DataFrame\n",
    "# Apply the generalized preprocessing to the dataframe\n",
    "df_processed = df.apply(preprocess_data_generalized, axis=1)\n",
    "\n",
    "# Drop rows where preprocessing failed (None values)\n",
    "df_processed = df_processed.dropna()\n",
    "\n",
    "# Convert the processed Series to a DataFrame\n",
    "df_final = pd.DataFrame(df_processed.tolist())\n",
    "\n",
    "# Display the first few rows of the processed data\n",
    "df_final.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7055ba3-de76-4271-b79c-53b97b803a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '<gender> Female </gender> <age> 32 years old days </age> <debt> 0 units </debt> <married> No </married> <bank_customer> No </bank_customer> <industry> 18 </industry> <ethnicity> Unknown </ethnicity> <years_employed> 4542.0 days days </years_employed> <prior_default> No </prior_default> <employed> No </employed> <credit_score> Unknown </credit_score> <drivers_license> Yes </drivers_license> <citizen> Unknown </citizen> <zip_code> Unknown </zip_code> <income> 427500.0 units </income>', 'label': '<approved> Yes </approved> <reason> This application was approved due to Client Number, Age Days, Property Ownership, Employment Duration Days, Car Ownership, Occupation, Housing Type, Marital Status, Education Level, Family Size, Annual Income, Number of Children, Income Category. </reason>'}\n"
     ]
    }
   ],
   "source": [
    "# Convert the first row to a dictionary for better readability\n",
    "first_row_dict = df_final.iloc[0].to_dict()\n",
    "print(first_row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41a97b2a-5ea0-4c90-b4ba-0c05e83f785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "new_model = \"/opt/notebooks/Chatbot-Credit-Card/models/llama-3.2-3b-CC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "765187fa-57b1-4600-90ae-78bb4a442e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dtype = torch.float16\n",
    "attn_implementation = \"eager\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfaa4237-421d-4b1b-8322-2d6728f97d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8a285bafd54ae5ac97effd356a2150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      3\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[1;32m      6\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_implementation\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Load tokenizer\u001b[39;00m\n\u001b[1;32m     17\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(base_model, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/CC-Chatbot/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/CC-Chatbot/lib/python3.12/site-packages/transformers/modeling_utils.py:3452\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3449\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3452\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3454\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3455\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3456\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m/opt/conda/envs/CC-Chatbot/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:82\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n\u001b[1;32m     81\u001b[0m bnb_multibackend_is_enabled \u001b[38;5;241m=\u001b[39m is_bitsandbytes_multi_backend_available()\n\u001b[0;32m---> 82\u001b[0m \u001b[43mvalidate_bnb_backend_availability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_tf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_flax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m sure the weights are in PyTorch format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/CC-Chatbot/lib/python3.12/site-packages/transformers/integrations/bitsandbytes.py:558\u001b[0m, in \u001b[0;36mvalidate_bnb_backend_availability\u001b[0;34m(raise_exception)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bitsandbytes_multi_backend_available():\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _validate_bnb_multi_backend_availability(raise_exception)\n\u001b[0;32m--> 558\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_validate_bnb_cuda_backend_availability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_exception\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/CC-Chatbot/lib/python3.12/site-packages/transformers/integrations/bitsandbytes.py:536\u001b[0m, in \u001b[0;36m_validate_bnb_cuda_backend_availability\u001b[0;34m(raise_exception)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_exception:\n\u001b[1;32m    535\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(log_msg)\n\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(log_msg)\n\u001b[1;32m    538\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarning(log_msg)\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend"
     ]
    }
   ],
   "source": [
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a905ad7-0c94-474c-8bf0-fd0ce3947412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(df_final)\n",
    "dataset['text'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628662d7-9884-4cd1-91a6-b98ee2ed8ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "modules = find_all_linear_names(model)\n",
    "print(modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646c87a7-42d3-4f1e-8398-33966ca2ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules\n",
    ")\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81be27d-7296-4c4c-9dbc-f3831088680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparamter\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    report_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07efe722-52f0-48e6-b245-45d1f5e99dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Assuming dataset is a Dataset object with columns 'text' and 'label'\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)  # Split into 80% train, 20% test\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d699e7-f389-4b8b-896d-82f8cfe0d6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Callback to save model every 5 epochs\n",
    "class SaveEveryNEpochsCallback(TrainerCallback):\n",
    "    def __init__(self, save_every_n_epochs, output_dir):\n",
    "        self.save_every_n_epochs = save_every_n_epochs\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def on_epoch_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        if state.epoch % self.save_every_n_epochs == 0:\n",
    "            model_save_path = os.path.join(self.output_dir, f\"checkpoint-{int(state.epoch)}-epochs\")\n",
    "            kwargs[\"model\"].save_pretrained(model_save_path)\n",
    "            kwargs[\"tokenizer\"].save_pretrained(model_save_path)\n",
    "            print(f\"Model saved at {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec94fda3-724c-46a0-b622-aa792699d854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the training arguments for the single epoch run\n",
    "single_epoch_training_arguments = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,  # Only run for one epoch first\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "# Train for one epoch and save as new_model + \"-1-epoch\"\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=512,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=single_epoch_training_arguments,\n",
    "    packing=False,\n",
    ")\n",
    "trainer.train()\n",
    "model.save_pretrained(new_model + \"-1-epoch\")\n",
    "tokenizer.save_pretrained(new_model + \"-1-epoch\")\n",
    "print(f\"Model saved after 1 epoch at {new_model + '-1-epoch'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110c3802-f9a9-4b41-a0b8-30752f92edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "early_stopping_patience = 5  # Number of eval steps with no improvement before stopping\n",
    "\n",
    "# Now, reconfigure for full training with 100 epochs and save every 5 epochs\n",
    "full_training_arguments = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=100,  # Set to full training epochs\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    report_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb78c1e-25d0-4fd8-8a90-00b835010ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full training with periodic saving\n",
    "trainer.args = full_training_arguments  # Update the training arguments\n",
    "trainer.add_callback(SaveEveryNEpochsCallback(save_every_n_epochs=5, output_dir=new_model))\n",
    "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=early_stopping_patience))\n",
    "\n",
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf37b01f-073b-4f33-8f5c-e7a4b921a15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction tailored to credit card approval context\n",
    "instruction = \"\"\"You are a highly knowledgeable financial advisor specializing in credit card approvals. \n",
    "    Be informative, polite, and provide clear responses to any queries regarding credit approval decisions.\n",
    "    \"\"\"\n",
    "\n",
    "# Example message (user asking about credit card approval)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": instruction},\n",
    "    {\"role\": \"user\", \"content\": \"Can I know why my credit card application was rejected? My age is 30, income is $40,000, and credit score is 580.\"}\n",
    "]\n",
    "\n",
    "# Generate the prompt using the chat template (assuming tokenizer.apply_chat_template is a custom method for your setup)\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Tokenize the prompt\n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "# Generate model outputs (adjusting parameters if necessary)\n",
    "outputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n",
    "\n",
    "# Decode the model's response\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the assistant's response (assuming the response begins after the 'assistant' token)\n",
    "print(text.split(\"assistant\")[1])\n",
    "\n",
    "# Save the fine-tuned model and tokenizer for future use\n",
    "model.save_pretrained(new_model)\n",
    "tokenizer.save_pretrained(new_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d6943f-4fdb-4704-9ada-897a202c9fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example custom prompt provided by the user\n",
    "custom_prompt = \"Age: 27.83, CreditScore: 5, Income: 3, YearsEmployed: 3.75, Gender: Male, Married: Yes, Industry: Industrials, Ethnicity: White, PriorDefault: Yes, Employed: Yes\"\n",
    "\n",
    "# You don't need a system message if you are simply testing this input directly\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": instruction},\n",
    "    {\"role\": \"user\", \"content\": custom_prompt}\n",
    "]\n",
    "\n",
    "# Generate the prompt using the chat template (if using custom chat template generation)\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Tokenize the custom prompt\n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "# Generate output from the model\n",
    "outputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n",
    "\n",
    "# Decode the model's response\n",
    "response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the assistant's response\n",
    "print(response_text.split(\"assistant\")[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dece5479-43bc-4ff3-a1e5-fd92861b720a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CC-Chatbot)",
   "language": "python",
   "name": "cc-chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
