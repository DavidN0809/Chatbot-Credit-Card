{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb87fd14-fae3-48c2-b6a7-b99e7537f22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import uvicorn\n",
    "import threading\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "# Initialize FastAPI\n",
    "app = FastAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbc1db0-ecd9-4e5e-b825-153a8b2ccc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define paths\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-8B-Instruct\"\n",
    "# ADAPTER_PATH = \"/opt/notebooks/Chatbot-Credit-Card/backend/models/llama-3.2-3b-CC/final-1-epoch/\"\n",
    "ADAPTER_PATH = \"/opt/notebooks/Chatbot-Credit-Card/backend/models/llama-3.2-8b-CC/base-1-epoch/\"\n",
    "print(os.listdir(\"/opt/notebooks/Chatbot-Credit-Card/backend/models/\"))\n",
    "print(os.listdir(\"/opt/notebooks/Chatbot-Credit-Card/backend/models/llama-3.2-8b-CC/base-1-epoch/\"))\n",
    "\n",
    "assert os.path.exists(ADAPTER_PATH), f\"Path does not exist: {ADAPTER_PATH}\"\n",
    "assert os.path.isfile(os.path.join(ADAPTER_PATH, \"adapter_config.json\")), \"adapter_config.json is missing in the specified path.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a6dd36a-2bd1-472e-ad1e-4e92e8127aa8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BASE_MODEL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load tokenizer\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[43mBASE_MODEL\u001b[49m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load base model with 4-bit quantization (if applicable)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      9\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,  \u001b[38;5;66;03m# Adjust dtype as needed\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BASE_MODEL' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "\n",
    "# Load base model with 4-bit quantization (if applicable)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # Adjust dtype as needed\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "# Load the adapter weights into the base model\n",
    "model = PeftModel.from_pretrained(model, ADAPTER_PATH, trust_remote_code=True, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5796ff4a-df62-48ab-97ed-3abf4decf720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure model embeddings match the tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "# Debugging information\n",
    "print(\"Tokenizer vocab size:\", len(tokenizer))\n",
    "print(\"Model parameters:\", model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8593dc4-8c18-48ed-b324-05b4b503ee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have `tokenizer` and `model` initialized and loaded\n",
    "# Set a padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Set `eos_token` as the `pad_token`\n",
    "\n",
    "# Test the prompt directly\n",
    "def test_prompt():\n",
    "    # Define the sample input\n",
    "    test_prompt = (\n",
    "        \"I am a Male who owns a car (yes) and a house (yes). \"\n",
    "        \"I earn 427500.0 per year and am in Civil marriage. \"\n",
    "        \"My education level is Higher education, and I live in a Rented apartment. \"\n",
    "        \"I am 32 years old and have 0 children. \"\n",
    "        \"My employment duration is 12.44 years.\"\n",
    "    )\n",
    "    max_length = 100\n",
    "    temperature = 0.8\n",
    "\n",
    "    # Tokenize the input and move it to the GPU\n",
    "    inputs = tokenizer(\n",
    "        test_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    # Generate text using the model\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,  # Pass attention mask explicitly\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the output\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"Generated Text:\", generated_text)\n",
    "\n",
    "# Run the test\n",
    "test_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1239c19b-25cf-4111-84bd-fb5d6fb09500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start FastAPI in a separate thread\n",
    "def run_fastapi():\n",
    "    import uvicorn\n",
    "    from fastapi import FastAPI\n",
    "    from pydantic import BaseModel\n",
    "\n",
    "    # Define FastAPI app\n",
    "    app = FastAPI()\n",
    "\n",
    "    class PromptRequest(BaseModel):\n",
    "        prompt: str\n",
    "        max_length: int = 50\n",
    "        temperature: float = 0.7\n",
    "\n",
    "    @app.post(\"/generate/\")\n",
    "    async def generate_text(request: PromptRequest):\n",
    "        # Generate text using the model\n",
    "        inputs = tokenizer.encode(request.prompt, return_tensors=\"pt\").to(\"cuda\")  # Move input to GPU\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=request.max_length,\n",
    "            temperature=request.temperature,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return {\"response\": generated_text}\n",
    "\n",
    "    # Run the FastAPI app\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n",
    "\n",
    "# Start the server thread\n",
    "server_thread = threading.Thread(target=run_fastapi, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "# Wait for the server to start\n",
    "time.sleep(2)\n",
    "\n",
    "# Define the FastAPI server URL\n",
    "BASE_URL = \"http://127.0.0.1:8000\"\n",
    "\n",
    "# Define a sample input aligned with your cc data\n",
    "test_prompt = {\n",
    "    \"prompt\": (\n",
    "        \"I am a Male who owns a car (yes) and a house (yes). \"\n",
    "        \"I earn 427500.0 per year and am in Civil marriage. \"\n",
    "        \"My education level is Higher education, and I live in a Rented apartment. \"\n",
    "        \"I am 32 years old and have 0 children. \"\n",
    "        \"My employment duration is 12.44 years.\"\n",
    "    ),\n",
    "    \"max_length\": 100,  # Adjust max_length to match complexity\n",
    "    \"temperature\": 0.8  # Adjust temperature as needed\n",
    "}\n",
    "\n",
    "# Send a POST request to the FastAPI /generate/ endpoint\n",
    "response = requests.post(f\"{BASE_URL}/generate/\", json=test_prompt)\n",
    "\n",
    "# Check the response\n",
    "if response.status_code == 200:\n",
    "    print(\"Generated Text:\", response.json()[\"response\"])\n",
    "else:\n",
    "    print(f\"Error: {response.status_code} - {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0724ff77-b7ed-4f0a-a2a8-7a92907c209d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CC-Chatbot)",
   "language": "python",
   "name": "cc-chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
